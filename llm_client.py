"""
LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""
import os
import json
import time
from typing import Dict, Any, Optional
import google.generativeai as genai
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class LLMClient:
    """Gemini APIã‚’ä½¿ç”¨ã™ã‚‹LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        api_key = os.getenv('GOOGLE_API_KEY')
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
        
        model_name = os.getenv('GEMINI_MODEL', 'gemini-pro')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯gemini-pro
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.console = Console()
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡
        self.total_input_tokens = 0
        self.total_output_tokens = 0
    
    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            token_count = self.model.count_tokens(text)
            return token_count.total_tokens
        except Exception as e:
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯0ã‚’è¿”ã™
            self.console.print(f"[yellow]âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}[/yellow]")
            return 0
    
    def log_token_info(self, text: str, label: str):
        """ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        char_count = len(text)
        token_count = self.count_tokens(text)
        self.console.print(f"[dim]{label}: {char_count}æ–‡å­—, {token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
    
    def update_token_usage(self, input_tokens: int, output_tokens: int):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æ›´æ–°"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
    
    def show_token_summary(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        total_tokens = self.total_input_tokens + self.total_output_tokens
        self.console.print(f"\n[bold cyan]ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼:[/bold cyan]")
        self.console.print(f"[dim]å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_input_tokens:,}[/dim]")
        self.console.print(f"[dim]å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_output_tokens:,}[/dim]")
        self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}[/dim]")
    
    def generate_text(self, prompt: str, progress_description: Optional[str] = None) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if progress_description:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    TimeElapsedColumn(),
                    console=self.console,
                    transient=False
                ) as progress:
                    task = progress.add_task(progress_description, total=None)
                    start_time = time.time()
                    self.console.print(f"[dim]é–‹å§‹æ™‚åˆ»: {time.strftime('%H:%M:%S')}[/dim]")
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    prompt_token_count = self.count_tokens(prompt)
                    self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {len(prompt)}æ–‡å­—, {prompt_token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]ãƒ¢ãƒ‡ãƒ«: {self.model.model_name}[/dim]")
                    
                    # APIå‘¼ã³å‡ºã—
                    self.console.print("[dim]APIå‘¼ã³å‡ºã—ä¸­...[/dim]")
                    response = self.model.generate_content(prompt)
                    
                    elapsed_time = time.time() - start_time
                    response_length = len(response.text) if response.text else 0
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    response_token_count = self.count_tokens(response.text) if response.text else 0
                    total_tokens = prompt_token_count + response_token_count
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²
                    self.update_token_usage(prompt_token_count, response_token_count)
                    
                    self.console.print(f"[green]âœ“ å®Œäº†[/green] (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’)")
                    self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_length}æ–‡å­—, {response_token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {total_tokens} (å…¥åŠ›: {prompt_token_count}, å‡ºåŠ›: {response_token_count})[/dim]")
                    
                    return response.text
            else:
                response = self.model.generate_content(prompt)
                return response.text
        except Exception as e:
            if progress_description:
                elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
                self.console.print(f"[red]âœ— ã‚¨ãƒ©ãƒ¼ (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’): {str(e)}[/red]")
            raise Exception(f"LLM API error: {str(e)}")
    
    def generate_plot(self, setting_content: str) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        from prompt_templates import PLOT_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“– ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_size = len(PLOT_GENERATION_PROMPT) - len("{setting_content}")  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼åˆ†ã‚’é™¤ã
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(PLOT_GENERATION_PROMPT.replace('{setting_content}', ''))}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        
        prompt = PLOT_GENERATION_PROMPT.format(setting_content=setting_content)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        response = self.generate_text(prompt, "ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")
        
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        try:
            self.console.print("[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...[/dim]")
            # ```json ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸­èº«ã‚’æŠ½å‡º
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                # JSONãŒç›´æ¥è¿”ã•ã‚ŒãŸå ´åˆ
                json_str = response.strip()
            
            plot_data = json.loads(json_str)
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆã®çµ±è¨ˆã‚’è¡¨ç¤º
            total_episodes = sum(len(episodes) for episodes in plot_data.values())
            self.console.print(f"[green]âœ“ ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†![/green]")
            self.console.print(f"[dim]ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆ: {len(plot_data)}ç·¨, å…¨{total_episodes}è©±[/dim]")
            
            # ç”Ÿæˆã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚‚è¨˜éŒ²
            json_output = json.dumps(plot_data, ensure_ascii=False, indent=2)
            self.log_token_info(json_output, "ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆJSON")
            
            return plot_data
        except json.JSONDecodeError as e:
            self.console.print(f"[red]âœ— JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}[/red]")
            self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response[:200]}...[/dim]")
            raise Exception(f"Failed to parse JSON response: {str(e)}\nResponse: {response}")
    
    def generate_episode(self, setting_content: str, plot_content: str) -> str:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ"""
        from prompt_templates import EPISODE_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_size = len(EPISODE_GENERATION_PROMPT) - len("{setting_content}") - len("{plot_content}")
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(EPISODE_GENERATION_PROMPT.replace('{setting_content}', '').replace('{plot_content}', ''))}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        self.log_token_info(plot_content, "ãƒ—ãƒ­ãƒƒãƒˆ")
        
        prompt = EPISODE_GENERATION_PROMPT.format(
            setting_content=setting_content,
            plot_content=plot_content
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        episode_content = self.generate_text(prompt, "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆä¸­...")
        
        self.console.print(f"[green]âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†![/green]")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(episode_content, "ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return episode_content
