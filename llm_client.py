"""
LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""
import os
import json
import time
from typing import Dict, Any, Optional
import litellm
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class LLMClient:
    """LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, model_name: Optional[str] = None, temperature: Optional[float] = None, 
                 top_p: Optional[float] = None, config_prefix: str = "LLM"):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            temperature: Temperatureå€¤ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            top_p: Top-på€¤ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            config_prefix: ç’°å¢ƒå¤‰æ•°ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆä¾‹: "LLM", "PLOT", "EPISODE"ï¼‰
        """
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯geminiï¼‰
        self.provider = os.getenv('LLM_PROVIDER', 'gemini')
        
        # ãƒ¢ãƒ‡ãƒ«åã®å–å¾—ï¼ˆå„ªå…ˆåº¦: å¼•æ•° > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        default_models = {
            'openai': 'gpt-4o-mini',
            'gemini': 'gemini/gemini-2.5-flash',
            'anthropic': 'claude-3-haiku-20240307',
            'azure': 'gpt-4o-mini'
        }
        
        self.model_name = (
            model_name or 
            os.getenv(f'{config_prefix}_MODEL') or 
            os.getenv('LLM_MODEL') or 
            default_models.get(self.provider, 'gemini-2.5-flash')
        )
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆå„ªå…ˆåº¦: å¼•æ•° > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        self.temperature = (
            temperature if temperature is not None else
            float(os.getenv(f'{config_prefix}_TEMPERATURE') or 
                  os.getenv('LLM_TEMPERATURE') or '1.0')
        )
        
        self.top_p = (
            top_p if top_p is not None else
            float(os.getenv(f'{config_prefix}_TOP_P') or 
                  os.getenv('LLM_TOP_P') or '0.95')
        )
        
        # LiteLLMã®è¨­å®š
        self._setup_litellm()
        
        self.console = Console()
        
        # è¨­å®šæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.provider}, ãƒ¢ãƒ‡ãƒ«: {self.model_name}, Temperature: {self.temperature}, Top-p: {self.top_p}[/dim]")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡
        self.total_input_tokens = 0
        self.total_output_tokens = 0
    
    def _setup_litellm(self):
        """LiteLLMã®è¨­å®š"""
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã«è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºï¼‰
        if os.getenv('LITELLM_DEBUG'):
            litellm.set_verbose = True
        else:
            litellm.set_verbose = False
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã®API ã‚­ãƒ¼è¨­å®š
        if self.provider == 'openai':
            if not os.getenv('OPENAI_API_KEY'):
                raise ValueError("OPENAI_API_KEY environment variable is required for OpenAI provider")
        elif self.provider == 'gemini':
            if not os.getenv('GEMINI_API_KEY'):
                raise ValueError("GEMINI_API_KEY environment variable is required for Gemini provider")
        elif self.provider == 'anthropic':
            if not os.getenv('ANTHROPIC_API_KEY'):
                raise ValueError("ANTHROPIC_API_KEY environment variable is required for Anthropic provider")
        elif self.provider == 'azure':
            if not os.getenv('AZURE_API_KEY'):
                raise ValueError("AZURE_API_KEY environment variable is required for Azure provider")
            if not os.getenv('AZURE_API_BASE'):
                raise ValueError("AZURE_API_BASE environment variable is required for Azure provider")
    
    @classmethod
    def create_for_plot_generation(cls) -> 'LLMClient':
        """ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆç”¨ã®LLMClientã‚’ä½œæˆ"""
        return cls(config_prefix="PLOT")
    
    @classmethod
    def create_for_episode_generation(cls) -> 'LLMClient':
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆç”¨ã®LLMClientã‚’ä½œæˆ"""
        return cls(config_prefix="EPISODE")
    
    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            # LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            token_count = litellm.token_counter(model=self.model_name, text=text)
            return token_count
        except Exception as e:
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ¨å®šå€¤ã‚’è¿”ã™
            self.console.print(f"[yellow]âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}[/yellow]")
            # å¤§ã¾ã‹ãªæ¨å®šï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³ = ç´„4æ–‡å­—ï¼‰
            return len(text) // 4
    
    def log_token_info(self, text: str, label: str):
        """ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        char_count = len(text)
        token_count = self.count_tokens(text)
        self.console.print(f"[dim]{label}: {char_count}æ–‡å­—, {token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
    
    def update_token_usage(self, input_tokens: int, output_tokens: int):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æ›´æ–°"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
    
    def show_token_summary(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        total_tokens = self.total_input_tokens + self.total_output_tokens
        self.console.print(f"\n[bold cyan]ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼:[/bold cyan]")
        self.console.print(f"[dim]å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_input_tokens:,}[/dim]")
        self.console.print(f"[dim]å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_output_tokens:,}[/dim]")
        self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}[/dim]")
    
    def generate_text(self, prompt: str, progress_description: Optional[str] = None) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if progress_description:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    TimeElapsedColumn(),
                    console=self.console,
                    transient=False
                ) as progress:
                    task = progress.add_task(progress_description, total=None)
                    start_time = time.time()
                    self.console.print(f"[dim]é–‹å§‹æ™‚åˆ»: {time.strftime('%H:%M:%S')}[/dim]")
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    prompt_token_count = self.count_tokens(prompt)
                    self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {len(prompt)}æ–‡å­—, {prompt_token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]ãƒ¢ãƒ‡ãƒ«: {self.model_name}[/dim]")
                    
                    # APIå‘¼ã³å‡ºã—
                    self.console.print("[dim]APIå‘¼ã³å‡ºã—ä¸­...[/dim]")
                    
                    # LiteLLMã‚’ä½¿ç”¨ã—ã¦APIã‚’å‘¼ã³å‡ºã—
                    # Geminiã®å®‰å…¨è¨­å®šã‚’èª¿æ•´ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç·©å’Œï¼‰
                    extra_params = {}
                    if self.provider == 'gemini':
                        # ç’°å¢ƒå¤‰æ•°ã§Geminiã®å®‰å…¨è¨­å®šã‚’åˆ¶å¾¡
                        if os.getenv('GEMINI_SAFETY_DISABLED', 'false').lower() == 'true':
                            extra_params["safety_settings"] = [
                                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                            ]
                            self.console.print(f"[dim]Geminiå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–[/dim]")
                    
                    response = litellm.completion(
                        model=self.model_name,
                        messages=[{"role": "user", "content": prompt}],
                        temperature=self.temperature,
                        top_p=self.top_p,
                        **extra_params
                    )
                    
                    elapsed_time = time.time() - start_time
                    response_text = response.choices[0].message.content
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
                    if response_text is None:
                        # finish_reasonã‚’ç¢ºèªã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                        finish_reason = response.choices[0].finish_reason if response.choices else "unknown"
                        
                        if finish_reason == 'content_filter':
                            self.console.print(f"[red]âœ— ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ[/red]")
                            self.console.print(f"[yellow]âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š[/yellow]")
                            self.console.print(f"[yellow]  - ã‚ˆã‚Šç©ã‚„ã‹ãªè¡¨ç¾ã«å¤‰æ›´[/yellow]")
                            self.console.print(f"[yellow]  - æš´åŠ›çš„ã€æ€§çš„ã€ã¾ãŸã¯ä¸é©åˆ‡ãªå†…å®¹ã‚’å‰Šé™¤[/yellow]")
                            self.console.print(f"[yellow]  - ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆgemini-1.5-flash ãªã©ï¼‰ã‚’è©¦ã™[/yellow]")
                            self.console.print(f"[yellow]  - OpenAIãªã©ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã‚‹[/yellow]")
                            
                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã‚’æ”¯æ´
                            if os.getenv('LITELLM_LOG') == 'DEBUG':
                                prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
                                self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­500æ–‡å­—: {prompt_preview}[/dim]")
                            
                            raise Exception(f"Content filtered by safety mechanisms (reason: {finish_reason})")
                        else:
                            self.console.print(f"[red]âœ— API ã‹ã‚‰ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ (finish_reason: {finish_reason})[/red]")
                            self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}[/dim]")
                            raise Exception(f"API returned None response (finish_reason: {finish_reason})")
                    
                    response_length = len(response_text)
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    response_token_count = self.count_tokens(response_text)
                    total_tokens = prompt_token_count + response_token_count
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å–å¾—ã§ãã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ï¼‰
                    if hasattr(response, 'usage') and response.usage:
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                    else:
                        input_tokens = prompt_token_count
                        output_tokens = response_token_count
                    
                    self.update_token_usage(input_tokens, output_tokens)
                    
                    self.console.print(f"[green]âœ“ å®Œäº†[/green] (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’)")
                    self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_length}æ–‡å­—, {output_tokens}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_tokens + output_tokens} (å…¥åŠ›: {input_tokens}, å‡ºåŠ›: {output_tokens})[/dim]")
                    
                    return response_text
            else:
                # Geminiã®å®‰å…¨è¨­å®šã‚’èª¿æ•´ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç·©å’Œï¼‰
                extra_params = {}
                if self.provider == 'gemini':
                    # ç’°å¢ƒå¤‰æ•°ã§Geminiã®å®‰å…¨è¨­å®šã‚’åˆ¶å¾¡
                    if os.getenv('GEMINI_SAFETY_DISABLED', 'false').lower() == 'true':
                        extra_params["safety_settings"] = [
                            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                        ]
                        self.console.print(f"[dim]Geminiå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–[/dim]")
                
                response = litellm.completion(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=self.temperature,
                    top_p=self.top_p,
                    **extra_params
                )
                response_text = response.choices[0].message.content
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
                if response_text is None:
                    # finish_reasonã‚’ç¢ºèªã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                    finish_reason = response.choices[0].finish_reason if response.choices else "unknown"
                    
                    if finish_reason == 'content_filter':
                        self.console.print(f"[red]âœ— ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ[/red]")
                        self.console.print(f"[yellow]âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š[/yellow]")
                        self.console.print(f"[yellow]  - ã‚ˆã‚Šç©ã‚„ã‹ãªè¡¨ç¾ã«å¤‰æ›´[/yellow]")
                        self.console.print(f"[yellow]  - æš´åŠ›çš„ã€æ€§çš„ã€ã¾ãŸã¯ä¸é©åˆ‡ãªå†…å®¹ã‚’å‰Šé™¤[/yellow]")
                        self.console.print(f"[yellow]  - ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆgemini-1.5-flash ãªã©ï¼‰ã‚’è©¦ã™[/yellow]")
                        self.console.print(f"[yellow]  - OpenAIãªã©ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã‚‹[/yellow]")
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã‚’æ”¯æ´
                        if os.getenv('LITELLM_LOG') == 'DEBUG':
                            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
                            self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­500æ–‡å­—: {prompt_preview}[/dim]")
                        
                        raise Exception(f"Content filtered by safety mechanisms (reason: {finish_reason})")
                    else:
                        self.console.print(f"[red]âœ— API ã‹ã‚‰ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ (finish_reason: {finish_reason})[/red]")
                        self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}[/dim]")
                        raise Exception(f"API returned None response (finish_reason: {finish_reason})")
                
                return response_text
        except Exception as e:
            if progress_description:
                elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
                self.console.print(f"[red]âœ— ã‚¨ãƒ©ãƒ¼ (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’): {str(e)}[/red]")
                
                # LiteLLMã®è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
                if hasattr(e, 'response') and e.response:
                    self.console.print(f"[dim]HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {e.response.status_code}[/dim]")
                    self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}[/dim]")
                
                # APIã‚­ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ç¢ºèªã‚’ä¿ƒã™
                self.console.print(f"[yellow]âš ï¸ è¨­å®šç¢ºèª: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼={self.provider}, ãƒ¢ãƒ‡ãƒ«={self.model_name}[/yellow]")
                
            raise Exception(f"LLM API error: {str(e)}")
    
    def generate_plot(self, setting_content: str, target_arc: str = None) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        from prompt_templates import PLOT_GENERATION_PROMPT, ARC_SPECIFIC_PLOT_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“– ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        if target_arc:
            # ç‰¹å®šã®ã‚¢ãƒ¼ã‚¯ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
            self.console.print(f"[dim]å¯¾è±¡ç·¨: {target_arc}[/dim]")
            template = ARC_SPECIFIC_PLOT_GENERATION_PROMPT
            prompt = template.format(setting_content=setting_content, target_arc=target_arc)
        else:
            # å…¨ä½“ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
            template = PLOT_GENERATION_PROMPT
            prompt = template.format(setting_content=setting_content)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_size = len(template) - len("{setting_content}") - (len("{target_arc}") if target_arc else 0)
        template_clean = template.replace('{setting_content}', '').replace('{target_arc}', '' if target_arc else '')
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(template_clean)}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        response = self.generate_text(prompt, "ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")
        
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        try:
            self.console.print("[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...[/dim]")
            # ```json ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸­èº«ã‚’æŠ½å‡º
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                # JSONãŒç›´æ¥è¿”ã•ã‚ŒãŸå ´åˆ
                json_str = response.strip()
            
            plot_data = json.loads(json_str)
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆã®çµ±è¨ˆã‚’è¡¨ç¤º
            total_episodes = sum(len(episodes) for episodes in plot_data.values())
            self.console.print(f"[green]âœ“ ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†![/green]")
            self.console.print(f"[dim]ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆ: {len(plot_data)}ç·¨, å…¨{total_episodes}è©±[/dim]")
            
            # ç”Ÿæˆã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚‚è¨˜éŒ²
            json_output = json.dumps(plot_data, ensure_ascii=False, indent=2)
            self.log_token_info(json_output, "ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆJSON")
            
            return plot_data
        except json.JSONDecodeError as e:
            self.console.print(f"[red]âœ— JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}[/red]")
            self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response[:200]}...[/dim]")
            raise Exception(f"Failed to parse JSON response: {str(e)}\nResponse: {response}")
    
    def generate_episode(self, setting_content: str, plot_content: str) -> str:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ"""
        from prompt_templates import EPISODE_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_size = len(EPISODE_GENERATION_PROMPT) - len("{setting_content}") - len("{plot_content}")
        template_clean = EPISODE_GENERATION_PROMPT.replace('{setting_content}', '').replace('{plot_content}', '')
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(template_clean)}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        self.log_token_info(plot_content, "ãƒ—ãƒ­ãƒƒãƒˆ")
        
        prompt = EPISODE_GENERATION_PROMPT.format(
            setting_content=setting_content,
            plot_content=plot_content
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        episode_content = self.generate_text(prompt, "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆä¸­...")
        
        self.console.print(f"[green]âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†![/green]")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(episode_content, "ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return episode_content
    
    def generate_episode_with_context(self, setting_content: str, arc: str, episode: int, 
                                      plot_data: Dict[str, Any]) -> str:
        """
        å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ã‚’å«ã‚ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        
        Args:
            setting_content: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
            arc: ç·¨å
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            plot_data: ãƒ—ãƒ­ãƒƒãƒˆå…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æœ¬æ–‡
        """
        from prompt_templates import EPISODE_GENERATION_WITH_CONTEXT_PROMPT, EPISODE_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå‰å¾Œæƒ…å ±ä»˜ãï¼‰[/bold blue]")
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—
        current_plot = plot_data.get(arc, {}).get(str(episode), "")
        if not current_plot:
            raise ValueError(f"Episode {episode} not found in arc '{arc}'")
        
        # å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ãƒ™ãƒ¼ã‚¹ï¼‰
        previous_plot = self._get_adjacent_episode_plot_by_file_order(plot_data, arc, episode, "previous")
        next_plot = self._get_adjacent_episode_plot_by_file_order(plot_data, arc, episode, "next")
        
        # å‰å¾Œã®æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãªã„å ´åˆã¯é€šå¸¸ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
        if previous_plot or next_plot:
            template = EPISODE_GENERATION_WITH_CONTEXT_PROMPT
            prompt = template.format(
                setting_content=setting_content,
                previous_plot=previous_plot,
                current_plot=current_plot,
                next_plot=next_plot
            )
        else:
            # å‰å¾Œã®æƒ…å ±ãŒãªã„å ´åˆã¯å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
            template = EPISODE_GENERATION_PROMPT
            prompt = template.format(
                setting_content=setting_content,
                plot_content=current_plot
            )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_placeholders = len("{setting_content}") + len("{previous_plot}") + len("{current_plot}") + len("{next_plot}")
        template_size = len(template) - template_placeholders
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—[/dim]")
        
        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        self.log_token_info(current_plot, "ç¾åœ¨ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        if previous_plot:
            self.log_token_info(previous_plot, "å‰ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        if next_plot:
            self.log_token_info(next_plot, "æ¬¡ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        episode_content = self.generate_text(prompt, "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆä¸­...")
        
        self.console.print(f"[green]âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†![/green]")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(episode_content, "ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return episode_content
    
    def _get_adjacent_episode_plot(self, plot_data: Dict[str, Any], arc: str, episode: int, 
                                   label: str) -> str:
        """
        éš£æ¥ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—
        
        Args:
            plot_data: ãƒ—ãƒ­ãƒƒãƒˆå…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿
            arc: ç·¨å
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            label: ãƒ­ã‚°ç”¨ã®ãƒ©ãƒ™ãƒ«ï¼ˆ"å‰ã®è©±"ã€"æ¬¡ã®è©±"ãªã©ï¼‰
        
        Returns:
            ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
        """
        if episode <= 0:
            return ""
        
        arc_data = plot_data.get(arc, {})
        episode_plot = arc_data.get(str(episode), "")
        
        if episode_plot:
            self.console.print(f"[dim]{label}ã®æƒ…å ±ã‚’å–å¾—: Episode {episode}[/dim]")
        else:
            self.console.print(f"[dim]{label}ã®æƒ…å ±: ãªã—[/dim]")
        
        return episode_plot

    def _get_adjacent_episode_plot_by_file_order(self, plot_data: Dict[str, Any], 
                                                 current_arc: str, current_episode: int, 
                                                 direction: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«åŸºã¥ã„ã¦éš£æ¥ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—
        chapteræ¦‚å¿µã‚’è€ƒæ…®ã—ã¦ç·¨ã‚’ã¾ãŸã„ã å‰å¾Œåˆ¤å®šã‚’è¡Œã†
        
        Args:
            plot_data: ãƒ—ãƒ­ãƒƒãƒˆå…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿
            current_arc: ç¾åœ¨ã®ç·¨å
            current_episode: ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            direction: "previous" ã¾ãŸã¯ "next"
        
        Returns:
            éš£æ¥ã™ã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
        """
        # å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ï¼ˆarc, episodeï¼‰ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆã§å–å¾—
        all_episodes = []
        for arc_name, episodes in plot_data.items():
            for ep_num_str, plot in episodes.items():
                try:
                    ep_num = int(ep_num_str)
                    all_episodes.append((arc_name, ep_num, plot))
                except ValueError:
                    continue
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åé †ã§ã‚½ãƒ¼ãƒˆï¼ˆarcåã§ã‚½ãƒ¼ãƒˆã€ãã®å¾Œepisodeç•ªå·ã§ã‚½ãƒ¼ãƒˆï¼‰
        all_episodes.sort(key=lambda x: (x[0], x[1]))
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢
        current_index = None
        for i, (arc_name, ep_num, plot) in enumerate(all_episodes):
            if arc_name == current_arc and ep_num == current_episode:
                current_index = i
                break
        
        if current_index is None:
            self.console.print(f"[dim]ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ {current_arc}_{current_episode} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“[/dim]")
            return ""
        
        # å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å–å¾—
        if direction == "previous":
            target_index = current_index - 1
            label = "å‰ã®è©±"
        elif direction == "next":
            target_index = current_index + 1
            label = "æ¬¡ã®è©±"
        else:
            return ""
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
        if 0 <= target_index < len(all_episodes):
            target_arc, target_episode, target_plot = all_episodes[target_index]
            self.console.print(f"[dim]{label}ã®æƒ…å ±ã‚’å–å¾—: {target_arc}_{target_episode}[/dim]")
            return target_plot
        else:
            self.console.print(f"[dim]{label}ã®æƒ…å ±: ãªã—[/dim]")
            return ""
