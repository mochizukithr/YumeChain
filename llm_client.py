"""
LLM ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
"""
import os
import json
import time
from typing import Dict, Any, Optional, List
import litellm
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, TimeElapsedColumn

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

class LLMClient:
    """LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å¯¾å¿œã®LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, model_name: Optional[str] = None, temperature: Optional[float] = None, 
                 top_p: Optional[float] = None, config_prefix: str = "LLM"):
        """
        åˆæœŸåŒ–
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            temperature: Temperatureå€¤ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            top_p: Top-på€¤ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
            config_prefix: ç’°å¢ƒå¤‰æ•°ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼ˆä¾‹: "LLM", "PLOT", "EPISODE"ï¼‰
        """
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯geminiï¼‰
        self.provider = os.getenv('LLM_PROVIDER', 'gemini')
        
        # ãƒ¢ãƒ‡ãƒ«åã®å–å¾—ï¼ˆå„ªå…ˆåº¦: å¼•æ•° > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        default_models = {
            'openai': 'gpt-4o-mini',
            'gemini': 'gemini/gemini-2.5-flash',
            'anthropic': 'claude-3-haiku-20240307',
            'azure': 'gpt-4o-mini'
        }
        
        self.model_name = (
            model_name or 
            os.getenv(f'{config_prefix}_MODEL') or 
            os.getenv('LLM_MODEL') or 
            default_models.get(self.provider, 'gemini-2.5-flash')
        )
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆå„ªå…ˆåº¦: å¼•æ•° > ç’°å¢ƒå¤‰æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        self.temperature = (
            temperature if temperature is not None else
            float(os.getenv(f'{config_prefix}_TEMPERATURE') or 
                  os.getenv('LLM_TEMPERATURE') or '1.0')
        )
        
        self.top_p = (
            top_p if top_p is not None else
            float(os.getenv(f'{config_prefix}_TOP_P') or 
                  os.getenv('LLM_TOP_P') or '0.95')
        )
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
        self.enable_context_cache = os.getenv('ENABLE_CONTEXT_CACHE', 'false').lower() == 'true'
        self.cached_contexts = {}  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿å­˜
        
        # LiteLLMã®è¨­å®š
        self._setup_litellm()
        
        self.console = Console()
        
        # è¨­å®šæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›
        cache_status = "æœ‰åŠ¹" if self.enable_context_cache else "ç„¡åŠ¹"
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {self.provider}, ãƒ¢ãƒ‡ãƒ«: {self.model_name}, Temperature: {self.temperature}, Top-p: {self.top_p}, ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cache_status}[/dim]")
        
        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¿½è·¡
        self.total_input_tokens = 0
        self.total_output_tokens = 0
    
    def _setup_litellm(self):
        """LiteLLMã®è¨­å®š"""
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã«è©³ç´°æƒ…å ±ã‚’è¡¨ç¤ºï¼‰
        if os.getenv('LITELLM_DEBUG'):
            litellm.set_verbose = True
        else:
            litellm.set_verbose = False
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã®API ã‚­ãƒ¼è¨­å®š
        if self.provider == 'openai':
            if not os.getenv('OPENAI_API_KEY'):
                raise ValueError("OPENAI_API_KEY environment variable is required for OpenAI provider")
        elif self.provider == 'gemini':
            if not os.getenv('GEMINI_API_KEY'):
                raise ValueError("GEMINI_API_KEY environment variable is required for Gemini provider")
        elif self.provider == 'anthropic':
            if not os.getenv('ANTHROPIC_API_KEY'):
                raise ValueError("ANTHROPIC_API_KEY environment variable is required for Anthropic provider")
        elif self.provider == 'azure':
            if not os.getenv('AZURE_API_KEY'):
                raise ValueError("AZURE_API_KEY environment variable is required for Azure provider")
            if not os.getenv('AZURE_API_BASE'):
                raise ValueError("AZURE_API_BASE environment variable is required for Azure provider")
    
    @classmethod
    def create_for_plot_generation(cls) -> 'LLMClient':
        """ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆç”¨ã®LLMClientã‚’ä½œæˆ"""
        return cls(config_prefix="PLOT")
    
    @classmethod
    def create_for_episode_generation(cls) -> 'LLMClient':
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆç”¨ã®LLMClientã‚’ä½œæˆ"""
        return cls(config_prefix="EPISODE")
    
    def count_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            # LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            token_count = litellm.token_counter(model=self.model_name, text=text)
            return token_count
        except Exception as e:
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯æ¨å®šå€¤ã‚’è¿”ã™
            self.console.print(f"[yellow]âš ï¸ ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}[/yellow]")
            # å¤§ã¾ã‹ãªæ¨å®šï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³ = ç´„4æ–‡å­—ï¼‰
            return len(text) // 4
    
    def log_token_info(self, text: str, label: str):
        """ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        char_count = len(text)
        token_count = self.count_tokens(text)
        self.console.print(f"[dim]{label}: {char_count}æ–‡å­—, {token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
    
    def update_token_usage(self, input_tokens: int, output_tokens: int):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æ›´æ–°"""
        self.total_input_tokens += input_tokens
        self.total_output_tokens += output_tokens
    
    def show_token_summary(self):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        total_tokens = self.total_input_tokens + self.total_output_tokens
        self.console.print(f"\n[bold cyan]ğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼:[/bold cyan]")
        self.console.print(f"[dim]å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_input_tokens:,}[/dim]")
        self.console.print(f"[dim]å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³: {self.total_output_tokens:,}[/dim]")
        self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³: {total_tokens:,}[/dim]")
    
    def generate_text(self, prompt: str, progress_description: Optional[str] = None, 
                     cached_keys: List[str] = None) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            if progress_description:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    TimeElapsedColumn(),
                    console=self.console,
                    transient=False
                ) as progress:
                    task = progress.add_task(progress_description, total=None)
                    start_time = time.time()
                    self.console.print(f"[dim]é–‹å§‹æ™‚åˆ»: {time.strftime('%H:%M:%S')}[/dim]")
                    
                    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è€ƒæ…®ï¼‰
                    messages = self._build_messages_with_cache(prompt, cached_keys)
                    
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    prompt_token_count = self.count_tokens(prompt)
                    self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {len(prompt)}æ–‡å­—, {prompt_token_count}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]ãƒ¢ãƒ‡ãƒ«: {self.model_name}[/dim]")
                    
                    # APIå‘¼ã³å‡ºã—
                    self.console.print("[dim]APIå‘¼ã³å‡ºã—ä¸­...[/dim]")
                    
                    # LiteLLMã‚’ä½¿ç”¨ã—ã¦APIã‚’å‘¼ã³å‡ºã—
                    # Geminiã®å®‰å…¨è¨­å®šã‚’èª¿æ•´ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç·©å’Œï¼‰
                    extra_params = {}
                    if self.provider == 'gemini':
                        # ç’°å¢ƒå¤‰æ•°ã§Geminiã®å®‰å…¨è¨­å®šã‚’åˆ¶å¾¡
                        if os.getenv('GEMINI_SAFETY_DISABLED', 'false').lower() == 'true':
                            extra_params["safety_settings"] = [
                                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                            ]
                            self.console.print(f"[dim]Geminiå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–[/dim]")
                        
                        # Geminiã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã®å‡¦ç†
                        if cached_keys and self.enable_context_cache and self._is_context_cache_supported():
                            # TTL (Time To Live) ã‚’è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1æ™‚é–“ï¼‰
                            cache_ttl = os.getenv('GEMINI_CACHE_TTL', '3600')  # ç§’å˜ä½
                            extra_params["ttl"] = int(cache_ttl)
                            self.console.print(f"[dim]Geminiã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’ä½¿ç”¨ (TTL: {cache_ttl}ç§’)[/dim]")
                    
                    response = litellm.completion(
                        model=self.model_name,
                        messages=messages,
                        temperature=self.temperature,
                        top_p=self.top_p,
                        **extra_params
                    )
                    
                    elapsed_time = time.time() - start_time
                    response_text = response.choices[0].message.content
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
                    if response_text is None:
                        # finish_reasonã‚’ç¢ºèªã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                        finish_reason = response.choices[0].finish_reason if response.choices else "unknown"
                        
                        if finish_reason == 'content_filter':
                            self.console.print(f"[red]âœ— ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ[/red]")
                            self.console.print(f"[yellow]âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š[/yellow]")
                            self.console.print(f"[yellow]  - ã‚ˆã‚Šç©ã‚„ã‹ãªè¡¨ç¾ã«å¤‰æ›´[/yellow]")
                            self.console.print(f"[yellow]  - æš´åŠ›çš„ã€æ€§çš„ã€ã¾ãŸã¯ä¸é©åˆ‡ãªå†…å®¹ã‚’å‰Šé™¤[/yellow]")
                            self.console.print(f"[yellow]  - ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆgemini-1.5-flash ãªã©ï¼‰ã‚’è©¦ã™[/yellow]")
                            self.console.print(f"[yellow]  - OpenAIãªã©ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã‚‹[/yellow]")
                            
                            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã‚’æ”¯æ´
                            if os.getenv('LITELLM_LOG') == 'DEBUG':
                                prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
                                self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­500æ–‡å­—: {prompt_preview}[/dim]")
                            
                            raise Exception(f"Content filtered by safety mechanisms (reason: {finish_reason})")
                        else:
                            self.console.print(f"[red]âœ— API ã‹ã‚‰ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ (finish_reason: {finish_reason})[/red]")
                            self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}[/dim]")
                            raise Exception(f"API returned None response (finish_reason: {finish_reason})")
                    
                    response_length = len(response_text)
                    
                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                    response_token_count = self.count_tokens(response_text)
                    total_tokens = prompt_token_count + response_token_count
                    
                    # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’è¨˜éŒ²ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å–å¾—ã§ãã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ï¼‰
                    if hasattr(response, 'usage') and response.usage:
                        input_tokens = response.usage.prompt_tokens
                        output_tokens = response.usage.completion_tokens
                    else:
                        input_tokens = prompt_token_count
                        output_tokens = response_token_count
                    
                    self.update_token_usage(input_tokens, output_tokens)
                    
                    self.console.print(f"[green]âœ“ å®Œäº†[/green] (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’)")
                    self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response_length}æ–‡å­—, {output_tokens}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
                    self.console.print(f"[dim]åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {input_tokens + output_tokens} (å…¥åŠ›: {input_tokens}, å‡ºåŠ›: {output_tokens})[/dim]")
                    
                    return response_text
            else:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è€ƒæ…®ï¼‰
                messages = self._build_messages_with_cache(prompt, cached_keys)
                
                # Geminiã®å®‰å…¨è¨­å®šã‚’èª¿æ•´ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ç·©å’Œï¼‰
                extra_params = {}
                if self.provider == 'gemini':
                    # ç’°å¢ƒå¤‰æ•°ã§Geminiã®å®‰å…¨è¨­å®šã‚’åˆ¶å¾¡
                    if os.getenv('GEMINI_SAFETY_DISABLED', 'false').lower() == 'true':
                        extra_params["safety_settings"] = [
                            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"}
                        ]
                        self.console.print(f"[dim]Geminiå®‰å…¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–[/dim]")
                    
                    # Geminiã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ãªå ´åˆã®å‡¦ç†
                    if cached_keys and self.enable_context_cache and self._is_context_cache_supported():
                        # TTL (Time To Live) ã‚’è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1æ™‚é–“ï¼‰
                        cache_ttl = os.getenv('GEMINI_CACHE_TTL', '3600')  # ç§’å˜ä½
                        extra_params["ttl"] = int(cache_ttl)
                        self.console.print(f"[dim]Geminiã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’ä½¿ç”¨ (TTL: {cache_ttl}ç§’)[/dim]")
                
                response = litellm.completion(
                    model=self.model_name,
                    messages=messages,
                    temperature=self.temperature,
                    top_p=self.top_p,
                    **extra_params
                )
                response_text = response.choices[0].message.content
                
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã®æ¤œè¨¼
                if response_text is None:
                    # finish_reasonã‚’ç¢ºèªã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®å ´åˆã¯ç‰¹åˆ¥ãªå‡¦ç†
                    finish_reason = response.choices[0].finish_reason if response.choices else "unknown"
                    
                    if finish_reason == 'content_filter':
                        self.console.print(f"[red]âœ— ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã«ã‚ˆã£ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ[/red]")
                        self.console.print(f"[yellow]âš ï¸ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†…å®¹ã‚’ç¢ºèªã—ã€ä»¥ä¸‹ã‚’è©¦ã—ã¦ãã ã•ã„ï¼š[/yellow]")
                        self.console.print(f"[yellow]  - ã‚ˆã‚Šç©ã‚„ã‹ãªè¡¨ç¾ã«å¤‰æ›´[/yellow]")
                        self.console.print(f"[yellow]  - æš´åŠ›çš„ã€æ€§çš„ã€ã¾ãŸã¯ä¸é©åˆ‡ãªå†…å®¹ã‚’å‰Šé™¤[/yellow]")
                        self.console.print(f"[yellow]  - ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆgemini-1.5-flash ãªã©ï¼‰ã‚’è©¦ã™[/yellow]")
                        self.console.print(f"[yellow]  - OpenAIãªã©ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã‚‹[/yellow]")
                        
                        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã‚’æ”¯æ´
                        if os.getenv('LITELLM_LOG') == 'DEBUG':
                            prompt_preview = prompt[:500] + "..." if len(prompt) > 500 else prompt
                            self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…ˆé ­500æ–‡å­—: {prompt_preview}[/dim]")
                        
                        raise Exception(f"Content filtered by safety mechanisms (reason: {finish_reason})")
                    else:
                        self.console.print(f"[red]âœ— API ã‹ã‚‰ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¿”ã•ã‚Œã¾ã—ãŸ (finish_reason: {finish_reason})[/red]")
                        self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹è©³ç´°: {response}[/dim]")
                        raise Exception(f"API returned None response (finish_reason: {finish_reason})")
                
                return response_text
        except Exception as e:
            if progress_description:
                elapsed_time = time.time() - start_time if 'start_time' in locals() else 0
                self.console.print(f"[red]âœ— ã‚¨ãƒ©ãƒ¼ (æ‰€è¦æ™‚é–“: {elapsed_time:.1f}ç§’): {str(e)}[/red]")
                
                # LiteLLMã®è©³ç´°ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
                if hasattr(e, 'response') and e.response:
                    self.console.print(f"[dim]HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {e.response.status_code}[/dim]")
                    self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text}[/dim]")
                
                # APIã‚­ãƒ¼ã‚„ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ç¢ºèªã‚’ä¿ƒã™
                self.console.print(f"[yellow]âš ï¸ è¨­å®šç¢ºèª: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼={self.provider}, ãƒ¢ãƒ‡ãƒ«={self.model_name}[/yellow]")
                
            raise Exception(f"LLM API error: {str(e)}")
    
    def generate_plot(self, setting_content: str, target_arc: str = None) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ"""
        from prompt_templates import PLOT_GENERATION_PROMPT, ARC_SPECIFIC_PLOT_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“– ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦ç™»éŒ²
        self.cache_setting_content(setting_content)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        if self.enable_context_cache and self._is_context_cache_supported():
            if target_arc:
                # ç‰¹å®šã®ã‚¢ãƒ¼ã‚¯ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‰ˆï¼‰
                self.console.print(f"[dim]å¯¾è±¡ç·¨: {target_arc}[/dim]")
                prompt = f"""æŒ‡å®šã•ã‚ŒãŸç·¨ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å¯¾è±¡ç·¨: {target_arc}

ä»¥ä¸‹ã®æ¡ä»¶ã§ç‰©èªã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- æŒ‡å®šã•ã‚ŒãŸç·¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹
- ãã®ç·¨å†…ã®å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°ãªãƒ—ãƒ­ãƒƒãƒˆã‚’å«ã‚ã‚‹
- JSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
- å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯æ•°å€¤ã®ã‚­ãƒ¼ã§ç®¡ç†
- å½¢å¼ä¾‹ï¼š
{{
  "{target_arc}": {{
    "1": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰1ã®ãƒ—ãƒ­ãƒƒãƒˆè©³ç´°...",
    "2": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰2ã®ãƒ—ãƒ­ãƒƒãƒˆè©³ç´°..."
  }}
}}"""
            else:
                # å…¨ä½“ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç‰ˆï¼‰
                prompt = """å…¨ä½“ã®ç‰©èªãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®æ¡ä»¶ã§ç‰©èªã®ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
- è¤‡æ•°ã®ç·¨ã«åˆ†ã‘ã¦æ§‹æˆ
- å„ç·¨ã«è¤‡æ•°ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹
- å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®è©³ç´°ãªãƒ—ãƒ­ãƒƒãƒˆã‚’è¨˜è¼‰
- JSONå½¢å¼ã§å‡ºåŠ›ã™ã‚‹
- å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¯æ•°å€¤ã®ã‚­ãƒ¼ã§ç®¡ç†
- å½¢å¼ä¾‹ï¼š
{
  "ç¬¬ä¸€ç·¨": {
    "1": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰1ã®ãƒ—ãƒ­ãƒƒãƒˆè©³ç´°...",
    "2": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰2ã®ãƒ—ãƒ­ãƒƒãƒˆè©³ç´°..."
  },
  "ç¬¬äºŒç·¨": {
    "1": "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰1ã®ãƒ—ãƒ­ãƒƒãƒˆè©³ç´°..."
  }
}"""
            cached_keys = ["setting"]
        else:
            # å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if target_arc:
                # ç‰¹å®šã®ã‚¢ãƒ¼ã‚¯ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
                self.console.print(f"[dim]å¯¾è±¡ç·¨: {target_arc}[/dim]")
                template = ARC_SPECIFIC_PLOT_GENERATION_PROMPT
                prompt = template.format(setting_content=setting_content, target_arc=target_arc)
            else:
                # å…¨ä½“ã®ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
                template = PLOT_GENERATION_PROMPT
                prompt = template.format(setting_content=setting_content)
            cached_keys = None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèªï¼ˆå¾“æ¥ç‰ˆä½¿ç”¨æ™‚ã®ã¿ï¼‰
        if not (self.enable_context_cache and self._is_context_cache_supported()):
            template_size = len(template) - len("{setting_content}") - (len("{target_arc}") if target_arc else 0)
            template_clean = template.replace('{setting_content}', '').replace('{target_arc}', '' if target_arc else '')
            self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(template_clean)}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        response = self.generate_text(prompt, "ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...", cached_keys)
        
        # JSONéƒ¨åˆ†ã‚’æŠ½å‡º
        try:
            self.console.print("[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...[/dim]")
            # ```json ã§å›²ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ä¸­èº«ã‚’æŠ½å‡º
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            else:
                # JSONãŒç›´æ¥è¿”ã•ã‚ŒãŸå ´åˆ
                json_str = response.strip()
            
            plot_data = json.loads(json_str)
            
            # ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆã®çµ±è¨ˆã‚’è¡¨ç¤º
            total_episodes = sum(len(episodes) for episodes in plot_data.values())
            self.console.print(f"[green]âœ“ ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆå®Œäº†![/green]")
            self.console.print(f"[dim]ç”Ÿæˆã•ã‚ŒãŸæ§‹æˆ: {len(plot_data)}ç·¨, å…¨{total_episodes}è©±[/dim]")
            
            # ç”Ÿæˆã•ã‚ŒãŸJSONãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚‚è¨˜éŒ²
            json_output = json.dumps(plot_data, ensure_ascii=False, indent=2)
            self.log_token_info(json_output, "ç”Ÿæˆã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆJSON")
            
            return plot_data
        except json.JSONDecodeError as e:
            self.console.print(f"[red]âœ— JSONãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {str(e)}[/red]")
            self.console.print(f"[dim]ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹: {response[:200]}...[/dim]")
            raise Exception(f"Failed to parse JSON response: {str(e)}\nResponse: {response}")
    
    def generate_episode(self, setting_content: str, plot_content: str) -> str:
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ"""
        from prompt_templates import EPISODE_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™[/bold blue]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦ç™»éŒ²
        self.cache_setting_content(setting_content)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        template_size = len(EPISODE_GENERATION_PROMPT) - len("{setting_content}") - len("{plot_content}")
        template_clean = EPISODE_GENERATION_PROMPT.replace('{setting_content}', '').replace('{plot_content}', '')
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—, {self.count_tokens(template_clean)}ãƒˆãƒ¼ã‚¯ãƒ³[/dim]")
        
        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        self.log_token_info(plot_content, "ãƒ—ãƒ­ãƒƒãƒˆ")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        if self.enable_context_cache and self._is_context_cache_supported():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨æ™‚ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é›¢ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒƒãƒˆã«åŸºã¥ã„ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

{plot_content}

ç”Ÿæˆè¦ä»¶:
- æ—¥æœ¬èªã§æ›¸ã
- å°èª¬å½¢å¼ã§æ›¸ã
- ä¼šè©±ã¯ã€Œã€ã§å›²ã‚€
- åœ°ã®æ–‡ã§å¿ƒæƒ…ã‚„çŠ¶æ³ã‚’ä¸å¯§ã«æå†™ã™ã‚‹
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦è‡ªç„¶ãªé•·ã•ã«ã™ã‚‹ï¼ˆ3000-8000æ–‡å­—ç¨‹åº¦ï¼‰
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’è‡ªç„¶ã«ã™ã‚‹
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å€‹æ€§ã‚’æ´»ã‹ã™"""
            cached_keys = ["setting"]
        else:
            # å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = EPISODE_GENERATION_PROMPT.format(
                setting_content=setting_content,
                plot_content=plot_content
            )
            cached_keys = None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        episode_content = self.generate_text(prompt, "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆä¸­...", cached_keys)
        
        self.console.print(f"[green]âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†![/green]")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(episode_content, "ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return episode_content
    
    def generate_episode_with_context(self, setting_content: str, arc: str, episode: int, 
                                      plot_data: Dict[str, Any]) -> str:
        """
        å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æƒ…å ±ã‚’å«ã‚ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆ
        
        Args:
            setting_content: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹
            arc: ç·¨å
            episode: ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç•ªå·
            plot_data: ãƒ—ãƒ­ãƒƒãƒˆå…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æœ¬æ–‡
        """
        from prompt_templates import EPISODE_GENERATION_WITH_CONTEXT_PROMPT, EPISODE_GENERATION_PROMPT
        
        self.console.print("[bold blue]ğŸ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå‰å¾Œæƒ…å ±ä»˜ãï¼‰[/bold blue]")
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦ç™»éŒ²
        self.cache_setting_content(setting_content)
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—
        current_plot = plot_data.get(arc, {}).get(str(episode), "")
        if not current_plot:
            raise ValueError(f"Episode {episode} not found in arc '{arc}'")
        
        # å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ—ãƒ­ãƒƒãƒˆã‚’å–å¾—ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ãƒ™ãƒ¼ã‚¹ï¼‰
        previous_plot = self._get_adjacent_episode_plot_by_file_order(plot_data, arc, episode, "previous")
        next_plot = self._get_adjacent_episode_plot_by_file_order(plot_data, arc, episode, "next")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        if self.enable_context_cache and self._is_context_cache_supported():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨æ™‚ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†é›¢ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if previous_plot or next_plot:
                prompt = f"""ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒƒãƒˆæƒ…å ±ã«åŸºã¥ã„ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

å‰ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ:
{previous_plot if previous_plot else "ï¼ˆãªã—ï¼‰"}

ç¾åœ¨ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ:
{current_plot}

æ¬¡ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ:
{next_plot if next_plot else "ï¼ˆãªã—ï¼‰"}

ç”Ÿæˆè¦ä»¶:
- æ—¥æœ¬èªã§æ›¸ã
- å°èª¬å½¢å¼ã§æ›¸ã
- ä¼šè©±ã¯ã€Œã€ã§å›²ã‚€
- åœ°ã®æ–‡ã§å¿ƒæƒ…ã‚„çŠ¶æ³ã‚’ä¸å¯§ã«æå†™ã™ã‚‹
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦è‡ªç„¶ãªé•·ã•ã«ã™ã‚‹ï¼ˆ3000-8000æ–‡å­—ç¨‹åº¦ï¼‰
- å‰å¾Œã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã®é€£ç¶šæ€§ã‚’æ„è­˜ã™ã‚‹
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’è‡ªç„¶ã«ã™ã‚‹
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å€‹æ€§ã‚’æ´»ã‹ã™"""
            else:
                prompt = f"""ä»¥ä¸‹ã®ãƒ—ãƒ­ãƒƒãƒˆã«åŸºã¥ã„ã¦ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

{current_plot}

ç”Ÿæˆè¦ä»¶:
- æ—¥æœ¬èªã§æ›¸ã
- å°èª¬å½¢å¼ã§æ›¸ã
- ä¼šè©±ã¯ã€Œã€ã§å›²ã‚€
- åœ°ã®æ–‡ã§å¿ƒæƒ…ã‚„çŠ¶æ³ã‚’ä¸å¯§ã«æå†™ã™ã‚‹
- ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦è‡ªç„¶ãªé•·ã•ã«ã™ã‚‹ï¼ˆ3000-8000æ–‡å­—ç¨‹åº¦ï¼‰
- ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®æµã‚Œã‚’è‡ªç„¶ã«ã™ã‚‹
- ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å€‹æ€§ã‚’æ´»ã‹ã™"""
            cached_keys = ["setting"]
        else:
            # å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            if previous_plot or next_plot:
                template = EPISODE_GENERATION_WITH_CONTEXT_PROMPT
                prompt = template.format(
                    setting_content=setting_content,
                    previous_plot=previous_plot,
                    current_plot=current_plot,
                    next_plot=next_plot
                )
            else:
                # å‰å¾Œã®æƒ…å ±ãŒãªã„å ´åˆã¯å¾“æ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
                template = EPISODE_GENERATION_PROMPT
                prompt = template.format(
                    setting_content=setting_content,
                    plot_content=current_plot
                )
            cached_keys = None
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        if not (self.enable_context_cache and self._is_context_cache_supported()):
            template_placeholders = len("{setting_content}") + len("{previous_plot}") + len("{current_plot}") + len("{next_plot}")
            template_size = len(template if 'template' in locals() else EPISODE_GENERATION_PROMPT) - template_placeholders
            self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_size}æ–‡å­—[/dim]")
        
        # å„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(setting_content, "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
        self.log_token_info(current_plot, "ç¾åœ¨ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        if previous_plot:
            self.log_token_info(previous_plot, "å‰ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        if next_plot:
            self.log_token_info(next_plot, "æ¬¡ã®è©±ã®ãƒ—ãƒ­ãƒƒãƒˆ")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã®ã‚µã‚¤ã‚ºã‚’ç¢ºèª
        self.console.print(f"[dim]ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨å¾Œã®å…¨ä½“ã‚µã‚¤ã‚ºç¢ºèª[/dim]")
        
        episode_content = self.generate_text(prompt, "ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆä¸­...", cached_keys)
        
        self.console.print(f"[green]âœ“ ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç”Ÿæˆå®Œäº†![/green]")
        
        # ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã‚’ãƒ­ã‚°å‡ºåŠ›
        self.log_token_info(episode_content, "ç”Ÿæˆã•ã‚ŒãŸã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
        
        return episode_content
    
    def _is_context_cache_supported(self) -> bool:
        """ç¾åœ¨ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        # Claude 3.5 Sonnetã€Claude 3 Haikuã€Claude 3.5 Haikuã€Claude 3 Opus ãŒã‚µãƒãƒ¼ãƒˆ
        anthropic_cache_models = [
            'claude-3-5-sonnet-20241022',
            'claude-3-5-sonnet-20240620', 
            'claude-3-5-haiku-20241022',
            'claude-3-haiku-20240307',
            'claude-3-opus-20240229'
        ]
        
        # OpenAI GPT-4oã€GPT-4o-mini ã‚‚ã‚µãƒãƒ¼ãƒˆ
        openai_cache_models = [
            'gpt-4o',
            'gpt-4o-mini',
            'gpt-4o-2024-11-20',
            'gpt-4o-2024-08-06',
            'gpt-4o-mini-2024-07-18'
        ]
        
        # Google Gemini 1.5 Pro/Flashã€2.0 Flashã€2.5 Flash ãŒã‚µãƒãƒ¼ãƒˆ
        gemini_cache_models = [
            'gemini-1.5-pro',
            'gemini-1.5-flash',
            'gemini-2.0-flash',
            'gemini-2.5-flash',
            'gemini/gemini-1.5-pro',
            'gemini/gemini-1.5-flash',
            'gemini/gemini-2.0-flash',
            'gemini/gemini-2.5-flash'
        ]
        
        if self.provider == 'anthropic':
            return any(model in self.model_name for model in anthropic_cache_models)
        elif self.provider == 'openai':
            return any(model in self.model_name for model in openai_cache_models)
        elif self.provider == 'gemini':
            return any(model in self.model_name for model in gemini_cache_models)
        else:
            return False
    
    def _create_cached_message(self, content: str, cache_type: str = "ephemeral") -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ"""
        if not self._is_context_cache_supported():
            return {"role": "user", "content": content}
        
        if self.provider == 'anthropic':
            # Anthropic Claude ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å½¢å¼
            return {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": content,
                        "cache_control": {"type": cache_type}
                    }
                ]
            }
        elif self.provider == 'openai':
            # OpenAI ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å½¢å¼ï¼ˆå°†æ¥ã®ã‚µãƒãƒ¼ãƒˆã«å‚™ãˆã¦ï¼‰
            return {
                "role": "user", 
                "content": content
                # OpenAIã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å°†æ¥è¿½åŠ ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
            }
        elif self.provider == 'gemini':
            # Google Gemini ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å½¢å¼
            # Geminiã§ã¯cached_contentã¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’äº‹å‰ç™»éŒ²ã—ã€
            # ãã®å¾Œcached_content_token_countã‚’ä½¿ç”¨ã™ã‚‹æ–¹å¼
            return {
                "role": "user", 
                "content": content,
                "_cache_hint": True  # LiteLLMçµŒç”±ã§ã®å®Ÿè£…æ™‚ã®å‚è€ƒç”¨
            }
        else:
            return {"role": "user", "content": content}
    
    def cache_setting_content(self, setting_content: str, cache_key: str = "setting") -> None:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦ç™»éŒ²"""
        if not self.enable_context_cache:
            return
        
        if not self._is_context_cache_supported():
            self.console.print(f"[yellow]âš ï¸ {self.provider}/{self.model_name} ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“[/yellow]")
            return
        
        self.cached_contexts[cache_key] = setting_content
        token_count = self.count_tokens(setting_content)
        self.console.print(f"[green]âœ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦ç™»éŒ² ({token_count}ãƒˆãƒ¼ã‚¯ãƒ³)[/green]")
    
    def _build_messages_with_cache(self, prompt: str, cached_keys: List[str] = None) -> List[Dict[str, Any]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰"""
        messages = []
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
        if cached_keys and self.enable_context_cache and self._is_context_cache_supported():
            for key in cached_keys:
                if key in self.cached_contexts:
                    cached_content = self.cached_contexts[key]
                    cached_message = self._create_cached_message(cached_content)
                    messages.append(cached_message)
                    self.console.print(f"[dim]ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ '{key}' ã‚’ä½¿ç”¨[/dim]")
        
        # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        messages.append({"role": "user", "content": prompt})
        
        return messages
